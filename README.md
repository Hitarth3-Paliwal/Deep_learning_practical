# Deep_learning_practical
Comparative Study of Encoder-Decoder Architectures with  Attention Mechanisms on Image Captioning 

## 4.1 Task 1: Identify a Research Paper 
● Select a recent research paper (ACL, arXiv, IEEE, Springer, etc.) 
● Focus: Use of Encoder-Decoder with Attention or Transformer 
● Based on your selected task domain 

##  Task 2: Implement Encoder-Decoder without Attention 
● Use LSTM/GRU-based encoder-decoder architecture 
● Train on a small dataset (e.g., MS-COCO for image captioning, SQuAD for 
QA, IWSLT14 for translation) 
● Evaluate using: 
BLEU / ROUGE / METEOR / CIDEr (based on task) and Training time, 
inference speed 

## Task 3: Implement Encoder-Decoder with Attention 
● Add Attention(EMbedding layer details)  
● Use the same dataset and architecture as in Task 2 
● Visualize attention weights (for interpretability) 
● Compare performance improvements 
Evaluate using the same metrics 

##  Task 4: Implement Encoder-Decoder with Self-Attention 
● Implement a Transformer-based encoder-decoder 
● Use Multi-head Attention, Positional Encoding, and Layer Norms 
● Train on the same task and dataset 
● Evaluate using the same metrics 

Dataset:https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset
