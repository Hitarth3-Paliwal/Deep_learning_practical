{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## installing pycoco API"
      ],
      "metadata": {
        "id": "AdVm86Mc_kbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocoevalcap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XswEThSQkxbz",
        "outputId": "af654de8-58e4-4255-846f-60dab4357c10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installing dependcies"
      ],
      "metadata": {
        "id": "QpKWo2ZDAOOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchvision nltk pycocoevalcap numpy pillow\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"Dependencies installed and NLTK data downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itYZuADQnyox",
        "outputId": "75df1c7c-adcf-42c5-8b86-f397de6f9435"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.11/dist-packages (1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Dependencies installed and NLTK data downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubjlc3hHn-ya",
        "outputId": "1376d4a3-c7bf-414e-a255-0fc815e61b3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyc-EOS-rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs_fZH3MoI-L",
        "outputId": "3e6a1ead-ab6d-4147-e170-2e954b734d9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pyc-EOS-rouge (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pyc-EOS-rouge\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchvision nltk pycocoevalcap numpy pillow\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"Dependencies installed and NLTK data downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWshq3OVo6hY",
        "outputId": "ad5e89d3-5caf-45b2-9b67-f1bbd3dbd0d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.11/dist-packages (1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Dependencies installed and NLTK data downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL DATASET\n"
      ],
      "metadata": {
        "id": "Sm296qDjCKug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "import uuid\n",
        "\n",
        "# Vocabulary class\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=5):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.next_idx = 4\n",
        "\n",
        "    def build_vocabulary(self, captions_list):\n",
        "        counter = Counter()\n",
        "        for caption in captions_list:\n",
        "            tokens = word_tokenize(caption.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "        for word, count in counter.items():\n",
        "            if count >= self.freq_threshold and word not in self.stoi:\n",
        "                self.stoi[word] = self.next_idx\n",
        "                self.itos[self.next_idx] = word\n",
        "                self.next_idx += 1\n",
        "\n",
        "    def encode(self, caption):\n",
        "        tokens = word_tokenize(caption.lower())\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return [self.itos.get(idx, \"<UNK>\") for idx in indices]\n",
        "\n",
        "# COCO Dataset class\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, vocab, transform=None, max_len=20, subset_size=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            self.coco = json.load(f)\n",
        "\n",
        "        self.annotations = self.coco['annotations']\n",
        "        if subset_size:\n",
        "            self.annotations = self.annotations[:subset_size]\n",
        "\n",
        "        self.image_ids = {ann['image_id']: ann for ann in self.annotations}\n",
        "        self.image_filenames = {ann['image_id']: os.path.join(root_dir, f\"COCO_{'train2014' if 'train' in annotation_file else 'val2014'}_{ann['image_id']:012d}.jpg\") for ann in self.annotations}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann = self.annotations[idx]\n",
        "        img_id = ann['image_id']\n",
        "        caption = ann['caption']\n",
        "\n",
        "        img_path = self.image_filenames[img_id]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        tokens = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.encode(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "        else:\n",
        "            tokens += [self.vocab.stoi[\"<PAD>\"]] * (self.max_len - len(tokens))\n",
        "\n",
        "        return image, torch.tensor(tokens)\n",
        "\n",
        "# Encoder (CNN)\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "# Decoder (LSTM)\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions[:, :-1])\n",
        "        features = features.unsqueeze(1)\n",
        "        inputs = torch.cat((features, embeddings), dim=1)\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "    def generate_caption(self, features, max_len=20):\n",
        "        batch_size = features.size(0)\n",
        "        captions = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        states = None\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            hiddens, states = self.lstm(inputs, states)\n",
        "            outputs = self.linear(hiddens.squeeze(1))\n",
        "            _, predicted = outputs.max(1)\n",
        "            captions.append(predicted)\n",
        "            inputs = self.embed(predicted).unsqueeze(1)\n",
        "\n",
        "        captions = torch.stack(captions, 1)\n",
        "        return captions\n",
        "\n",
        "# Full Model\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = EncoderCNN(embed_size)\n",
        "        self.decoder = DecoderLSTM(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, data_loader, vocab, device):\n",
        "    model.eval()\n",
        "    gts = {}\n",
        "    res = {}\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, captions) in enumerate(data_loader):\n",
        "            images = images.to(device)\n",
        "            features = model.encoder(images)\n",
        "            predicted_ids = model.decoder.generate_caption(features)\n",
        "\n",
        "            for i in range(len(predicted_ids)):\n",
        "                pred_caption = vocab.decode(predicted_ids[i].cpu().numpy())\n",
        "                gt_caption = vocab.decode(captions[i].cpu().numpy())\n",
        "                pred_caption = [w for w in pred_caption if w not in [\"<PAD>\", \"<SOS>\", \"<EOS>\"]]\n",
        "                gt_caption = [w for w in gt_caption if w not in [\"<PAD>\", \"<SOS>\", \"<EOS>\"]]\n",
        "                img_id = str(uuid.uuid4())\n",
        "                gts[img_id] = [gt_caption]\n",
        "                res[img_id] = [pred_caption]\n",
        "\n",
        "    inference_time = time.time() - start_time\n",
        "    avg_inference_time = inference_time / len(data_loader.dataset)\n",
        "\n",
        "    scorers = {\n",
        "        \"Bleu\": Bleu(4),\n",
        "        \"Rouge\": Rouge(),\n",
        "        \"Meteor\": Meteor(),\n",
        "        \"Cider\": Cider()\n",
        "    }\n",
        "    scores = {}\n",
        "    for name, scorer in scorers.items():\n",
        "        score, _ = scorer.compute_score(gts, res)\n",
        "        if isinstance(score, list):\n",
        "            for i, sc in enumerate(score, 1):\n",
        "                scores[f\"{name}_{i}\"] = sc\n",
        "        else:\n",
        "            scores[name] = score\n",
        "\n",
        "    return scores, avg_inference_time\n",
        "\n",
        "# Setup vocabulary and datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "annotation_file = \"/content/coco_data/annotations/captions_train2014.json\"\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco = json.load(f)\n",
        "captions = [ann['caption'] for ann in coco['annotations'][:5000]]\n",
        "\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "vocab.build_vocabulary(captions)\n",
        "\n",
        "train_dataset = COCODataset(\n",
        "    root_dir=\"/content/coco_data/train2014\",\n",
        "    annotation_file=\"/content/coco_data/annotations/captions_train2014.json\",\n",
        "    vocab=vocab,\n",
        "    transform=transform,\n",
        "    subset_size=5000\n",
        ")\n",
        "val_dataset = COCODataset(\n",
        "    root_dir=\"/content/coco_data/val2014\",\n",
        "    annotation_file=\"/content/coco_data/annotations/captions_val2014.json\",\n",
        "    vocab=vocab,\n",
        "    transform=transform,\n",
        "    subset_size=500\n",
        ")\n",
        "\n",
        "print(\"Vocabulary and datasets initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3jbbINeo1yA",
        "outputId": "7d7499f5-bbe1-4221-e6c3-02f4c3ce7555"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary and datasets initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading COCO"
      ],
      "metadata": {
        "id": "RFyeLDQtB8U0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2-BBlefB8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def download_coco():\n",
        "    os.makedirs('/content/coco_data', exist_ok=True)\n",
        "    os.makedirs('/content/coco_data/train2014', exist_ok=True)\n",
        "    os.makedirs('/content/coco_data/val2014', exist_ok=True)\n",
        "    os.makedirs('/content/coco_data/annotations', exist_ok=True)\n",
        "\n",
        "    urls = [\n",
        "        ('http://images.cocodataset.org/zips/train2014.zip', '/content/coco_data/train2014.zip'),\n",
        "        ('http://images.cocodataset.org/zips/val2014.zip', '/content/coco_data/val2014.zip'),\n",
        "        ('http://images.cocodataset.org/annotations/annotations_trainval2014.zip', '/content/coco_data/annotations.zip')\n",
        "    ]\n",
        "\n",
        "    for url, dest in urls:\n",
        "        print(f\"Downloading {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(dest, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f\"Extracting {dest}...\")\n",
        "        with zipfile.ZipFile(dest, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/coco_data')\n",
        "        os.remove(dest)\n",
        "\n",
        "    print(\"COCO dataset downloaded and extracted.\")\n",
        "\n",
        "# Run download\n",
        "download_coco()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxrLWzuAqKPA",
        "outputId": "80d24562-8a3e-4345-b370-517bccf53e32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://images.cocodataset.org/zips/train2014.zip...\n",
            "Extracting /content/coco_data/train2014.zip...\n",
            "Downloading http://images.cocodataset.org/zips/val2014.zip...\n",
            "Extracting /content/coco_data/val2014.zip...\n",
            "Downloading http://images.cocodataset.org/annotations/annotations_trainval2014.zip...\n",
            "Extracting /content/coco_data/annotations.zip...\n",
            "COCO dataset downloaded and extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING EVALUATE\n"
      ],
      "metadata": {
        "id": "qqkpCcCTCSpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Assuming train_dataset, val_dataset, vocab, EncoderDecoder, evaluate_model are defined from Block 3\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "num_epochs = 1\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = EncoderDecoder(embed_size, hidden_size, len(vocab.stoi), vocab, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, vocab, criterion, optimizer, num_epochs, device):\n",
        "    training_times = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, captions in train_loader:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            outputs = model(images, captions)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        training_times.append(epoch_time)\n",
        "\n",
        "        val_scores, _ = evaluate_model(model, val_loader, vocab, device)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
        "        print(f\"Validation Scores: {val_scores}\")\n",
        "\n",
        "    avg_training_time = sum(training_times) / len(training_times)\n",
        "    return avg_training_time\n",
        "\n",
        "# Train\n",
        "avg_training_time = train_model(model, train_loader, val_loader, vocab, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "# Final evaluation\n",
        "final_scores, avg_inference_time = evaluate_model(model, val_loader, vocab, device)\n",
        "\n",
        "print(\"\\nFinal Evaluation Scores:\")\n",
        "for metric, score in final_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "print(f\"Average Training Time per Epoch: {avg_training_time:.2f}s\")\n",
        "print(f\"Average Inference Time per Image: {avg_inference_time:.4f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SKJS3B58znhN",
        "outputId": "237b6d6e-e0e5-4685-8bce-f4f9ef1e8223"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "EncoderDecoder.__init__() takes from 4 to 5 positional arguments but 6 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3a2468f49ed4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Initialize model, loss, optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<PAD>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EncoderDecoder.__init__() takes from 4 to 5 positional arguments but 6 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Assuming train_dataset, val_dataset, vocab, EncoderDecoder, evaluate_model are defined from Block 3\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "num_epochs = 1\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = EncoderDecoder(embed_size, hidden_size, len(vocab.stoi), vocab, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, vocab, criterion, optimizer, num_epochs, device):\n",
        "    training_times = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, captions in train_loader:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            outputs = model(images, captions)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        training_times.append(epoch_time)\n",
        "\n",
        "        val_scores, _ = evaluate_model(model, val_loader, vocab, device)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
        "        print(f\"Validation Scores: {val_scores}\")\n",
        "\n",
        "    avg_training_time = sum(training_times) / len(training_times)\n",
        "    return avg_training_time\n",
        "\n",
        "# Train\n",
        "avg_training_time = train_model(model, train_loader, val_loader, vocab, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "# Final evaluation\n",
        "final_scores, avg_inference_time = evaluate_model(model, val_loader, vocab, device)\n",
        "\n",
        "print(\"\\nFinal Evaluation Scores:\")\n",
        "for metric, score in final_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "print(f\"Average Training Time per Epoch: {avg_training_time:.2f}s\")\n",
        "print(f\"Average Inference Time per Image: {avg_inference_time:.4f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SEIL1b0Z06Im",
        "outputId": "cd0e4c28-4427-414e-dc9d-963dddcdec88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "EncoderDecoder.__init__() takes from 4 to 5 positional arguments but 6 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3a2468f49ed4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Initialize model, loss, optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<PAD>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EncoderDecoder.__init__() takes from 4 to 5 positional arguments but 6 were given"
          ]
        }
      ]
    }
  ]
}